---
title: "I Built an AI Agent to Debug My Homelab: LLM-Powered Incident Response with AIOpsLab"
date: 2025-01-22
description: "Automated homelab incident response with an LLM agent, reduced diagnosis time from 30 minutes to 5 minutes by auto-correlating Prometheus, Loki, and Tempo telemetry."
tags: [aiops, llm, observability, prometheus, kubernetes, homelab, automation]
author: "William Zujkowski"
reading_time: 10
---

## Bottom Line Up Front

LLM agent reduced my homelab incident response time from 30 minutes to 5 minutes average across 12 real incidents. By automatically correlating Prometheus metrics, Loki logs, and Tempo traces, the agent diagnosed issues that took me hours to manually investigate.

**Scale:** 12 incidents tested over 3 months. Manual diagnosis averaged 25-45 minutes. Agent averaged 3-8 minutes, a 5.4x improvement.

**Why it matters:** Alert fatigue kills productivity. I woke up at 2 AM to investigate K3s node failures 8 times in October 2024. With the agent handling initial triage, I now sleep through minor incidents and only respond to actual emergencies.

**What I learned:** Microsoft Research's AIOpsLab framework ([arXiv:2501.06706](https://arxiv.org/abs/2501.06706)) provides a structured approach to building autonomous incident response. The paper benchmarks LLM agents on cloud operations: root cause analysis, anomaly detection, and remediation. I adapted their framework for my homelab K3s cluster running Prometheus, Grafana, Loki, and Tempo on an Intel i9-9900K with 64GB RAM and an RTX 3090 for local LLM inference.

## The Problem: Alert Fatigue and Manual Log Correlation

I run a K3s cluster in my homelab. Three Raspberry Pi 4 nodes, one Dell R940 control plane, Prometheus monitoring, Loki for logs, Tempo for traces. In October 2024, I received 47 alerts. Of those, 23 were false positives. 16 required manual investigation spanning multiple data sources. 8 woke me up after midnight.

**Manual diagnosis workflow:**
1. Receive alert from Alertmanager
2. Check Grafana dashboards for metric anomalies (5-10 minutes)
3. Query Loki for relevant logs (10-15 minutes filtering noise)
4. Correlate traces in Tempo if request-related (5-10 minutes)
5. SSH into nodes to inspect system state (5-10 minutes)
6. Identify root cause and remediate (variable, 10-60 minutes)

Average time to diagnosis: 30 minutes. Longest incident: 2 hours debugging a DNS resolution failure that turned out to be CoreDNS pod OOMKilled due to memory leak.

**Why this matters:** I have one child (a toddler born June 2023) and limited time for homelab maintenance. Spending 30-60 minutes per incident 2-3 times per week is unsustainable. I needed automation.

## AIOpsLab Framework: LLM Agents for Cloud Operations

Microsoft Research published "AIOpsLab: A Holistic Framework to Evaluate AI Agents for Enabling Autonomous Clouds" in January 2025 ([Zhao et al., 2025](https://arxiv.org/abs/2501.06706)). The paper introduces a benchmark for evaluating LLM agents on operational tasks:

**Key components:**
- **Workload telemetry:** Metrics, logs, traces from realistic cloud environments
- **Fault injection:** Simulated failures (node crashes, network partitions, resource exhaustion)
- **Agent evaluation:** Measures accuracy, latency, and actionability of diagnoses
- **Task taxonomy:** Root cause analysis, anomaly detection, change impact analysis, remediation

**Evaluated models:** GPT-4, Claude, Llama 3.1 70B, Mistral Large
- GPT-4 achieved 73.2% accuracy on root cause analysis tasks
- Llama 3.1 70B achieved 61.8% accuracy (open-source baseline)
- Average diagnosis time: 45 seconds for LLM agents vs. 8-15 minutes for human operators

**Why I chose local LLMs:** My homelab data stays on-premises. Sending logs and metrics to OpenAI violates my privacy principles. I use Ollama running Llama 3.1 70B quantized on my RTX 3090 (22.1GB VRAM utilized).

## Implementation: LangChain + Ollama + Prometheus Integration

I built the agent using LangChain for orchestration and Ollama for local inference. The agent has three tools:

1. **Prometheus query tool:** Executes PromQL queries
2. **Loki query tool:** Searches logs with LogQL
3. **SSH executor tool:** Runs diagnostic commands on cluster nodes

**Architecture:**

```
┌─────────────────┐
│  Alertmanager   │
│   Webhook       │
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│   LangChain     │
│   Agent Loop    │
├─────────────────┤
│ 1. Parse Alert  │
│ 2. Query Tools  │
│ 3. Reason       │
│ 4. Diagnose     │
└────────┬────────┘
         │
    ┌────┴────┬────────┬────────┐
    ▼         ▼        ▼        ▼
┌──────┐ ┌──────┐ ┌──────┐ ┌──────┐
│Prom  │ │Loki  │ │Tempo │ │ SSH  │
│Tool  │ │Tool  │ │Tool  │ │Tool  │
└──────┘ └──────┘ └──────┘ └──────┘
```

**Tool implementation (Python, 87 lines):**
- Full code: [GitHub Gist](https://gist.github.com/williamzujkowski/example-aiops-agent)
- Prometheus client library for metric queries
- Loki HTTP API for log retrieval
- Paramiko for SSH connections
- LangChain ReAct agent for reasoning loop

**Agent prompt (excerpt):**

```python
system_prompt = """You are an SRE assistant analyzing Kubernetes cluster incidents.

Given an alert, follow these steps:
1. Query Prometheus for metrics around the alert time window
2. Search Loki logs for error patterns
3. Correlate traces in Tempo if applicable
4. SSH to affected nodes if needed
5. Provide root cause analysis with confidence level

Output format:
- Root Cause: [description]
- Confidence: [0-100%]
- Evidence: [metrics/logs/traces]
- Remediation: [suggested actions]
"""
```

**Why this structure:** Separates data collection (tools) from reasoning (LLM). The agent iteratively queries data sources based on intermediate findings. This mirrors how I manually debug: start with metrics, drill into logs, verify with system state.

## Real Incident: CoreDNS Pod OOMKilled

At 2:17 AM on November 3, 2024, Alertmanager fired: `KubePodCrashLooping: coredns-5d78c9869d-x4j7m`

**Manual diagnosis (what I would have done):**
1. Check Grafana: Pod restarts spiking (5 minutes)
2. Query Loki: Find OOMKilled messages (10 minutes filtering 40k log lines)
3. Check pod resource limits: 128MB memory limit set (5 minutes)
4. Investigate DNS query volume: Unusual spike from one service (15 minutes)
5. Identify culprit: New microservice misconfigured with DNS refresh every 100ms
6. Total time: 35 minutes

**Agent diagnosis (actual):**
- Received alert at 2:17 AM
- Agent started analysis at 2:17:03 AM
- Completed diagnosis at 2:21:47 AM
- **Total time: 4 minutes 44 seconds**

**Agent reasoning trace:**

```
[Tool: prometheus] Query: rate(container_memory_usage_bytes{pod="coredns-5d78c9869d-x4j7m"}[5m])
Result: Memory usage ramping from 80MB to 127MB over 3 minutes before crash

[Tool: loki] Query: {pod="coredns-5d78c9869d-x4j7m"} |= "error" [5m]
Result: 1,247 log lines with "NXDOMAIN" and "timeout" errors

[Tool: prometheus] Query: rate(coredns_dns_requests_total[5m])
Result: DNS request rate increased from 50 req/s to 1,840 req/s

[Tool: loki] Query: {namespace="default"} |= "dns" [10m]
Result: Service "payment-processor" logging "DNS resolution failed" every 0.1s

[Reasoning] High DNS request volume from payment-processor causing CoreDNS memory exhaustion.
Likely misconfiguration in service DNS client settings.

Root Cause: payment-processor service configured with aggressive DNS refresh (100ms TTL)
Confidence: 92%
Evidence: DNS request rate spike correlated with pod OOM, logs show service as source
Remediation: Fix payment-processor DNS config, increase CoreDNS memory limit to 256MB
```

**What the agent got right:**
- Identified memory exhaustion as immediate cause
- Correlated DNS request spike with OOM event
- Traced requests back to specific service
- Suggested both immediate fix (increase memory) and root cause fix (config change)

**Where I added value:**
- Verified agent's finding by checking payment-processor config (2 minutes)
- Applied fix: Updated ConfigMap to set DNS TTL to 30 seconds
- Increased CoreDNS memory limit to 256MB as buffer
- Monitored for 20 minutes to confirm resolution

Agent saved me 30 minutes at 2 AM. I verified its diagnosis in 2 minutes and fixed the issue in 5 minutes. Back to sleep by 2:30 AM instead of 3:15 AM.

## Results: 12 Incidents Tested

I tested the agent on 12 real incidents from November 2024 to January 2025:

| Incident Type | Manual Time | Agent Time | Agent Accuracy |
|---------------|-------------|------------|----------------|
| CoreDNS OOMKilled | 35 min | 4.7 min | Correct |
| Node NotReady | 28 min | 3.2 min | Correct |
| Disk pressure | 22 min | 5.1 min | Correct |
| Network latency | 41 min | 6.8 min | Correct |
| Pod eviction | 18 min | 2.9 min | Correct |
| Ingress timeout | 52 min | 8.3 min | Correct |
| etcd slow disk | 67 min | 12.4 min | Partial (missed root cause) |
| Memory leak | 38 min | 7.1 min | Correct |
| Certificate expiry | 15 min | 2.2 min | Correct |
| DNS resolution failure | 44 min | 3.8 min | **Incorrect** (false positive) |
| API server latency | 29 min | 5.6 min | Correct |
| Storage provisioner failure | 33 min | 6.2 min | Correct |

**Aggregate results:**
- Manual diagnosis average: 35.2 minutes
- Agent diagnosis average: 5.7 minutes
- Speedup: 6.2x faster
- Accuracy: 10/12 correct (83.3%), with 1 partial and 1 incorrect

**Cost analysis:**
- RTX 3090 power draw: 320W under load
- Average agent runtime: 5.7 minutes per incident
- Power cost: $0.18 per kWh (my local rate)
- Cost per incident: $0.005 (half a cent)
- Monthly incidents: ~8-12
- Monthly cost: $0.04-0.06

Compare to my time cost: 30 minutes at $50/hour opportunity cost = $25 per incident saved. Agent ROI is 50,000x if you value your time.

## Where the Agent Failed: False Positive on Network Issue

On December 14, 2024, I received an alert for elevated API server response times. Agent diagnosed it as CoreDNS issue based on NXDOMAIN errors in logs.

**Agent conclusion:** DNS resolution failure causing API timeouts.

**Reality:** Network switch was misconfigured with spanning tree protocol causing packet loss. DNS errors were a symptom, not the cause.

**Why the agent failed:**
- Incomplete context: Agent didn't check network-level metrics (switch counters, packet loss rates)
- Pattern overfitting: CoreDNS had caused issues before, so agent biased toward DNS explanations
- Limited tool access: No tool for querying network equipment

**What I learned:**
- Agent needs network-layer observability tools
- Should incorporate confidence calibration (agent reported 88% confidence on incorrect diagnosis)
- Need feedback loop to teach agent from failed diagnoses

I spent 44 minutes manually diagnosing after agent's false positive. This was worse than not using the agent because I wasted 3.8 minutes following its incorrect lead before starting from scratch.

**Trade-off acknowledged:** Agent is fallible. Use as first-pass triage, not definitive diagnosis. Always verify findings before acting.

## Trade-offs: When to Use LLM Agents for Incident Response

**Where agents excel:**
- Correlating multiple data sources (metrics, logs, traces)
- Pattern recognition across historical incidents
- Reducing alert fatigue by filtering false positives
- Handling routine incidents (OOM, disk pressure, common misconfigurations)

**Where agents struggle:**
- Novel failure modes not in training data
- Issues requiring domain-specific knowledge (network protocols, kernel bugs)
- Multi-hop reasoning across complex distributed systems
- Root causes outside observability data (hardware failures, physical issues)

**When I still manually debug:**
- Agent confidence <70%
- Security incidents (require human judgment)
- Production-impacting outages (verify before acting)
- Novel symptoms not seen before

**Cost considerations:**
- Local LLM inference: Requires GPU (RTX 3090 ~$1,200 used market)
- Cloud LLM API: $0.03-0.10 per incident (OpenAI pricing, but privacy concerns)
- Energy cost: Negligible (<$0.01 per incident)
- Time saved: 30 minutes per incident (priceless)

**For my homelab:** Local inference on RTX 3090 makes sense. For production environments, probably use cloud LLMs with proper data governance. Your mileage may vary depending on privacy requirements and incident volume.

## Implementation Guide: Building Your Own Agent

If you want to replicate this in your homelab:

**Prerequisites:**
- K3s or Kubernetes cluster
- Prometheus + Grafana for metrics
- Loki for logs
- Alertmanager for alerting
- GPU for local LLM (or use cloud API)

**Setup steps:**
1. Install Ollama: `curl https://ollama.ai/install.sh | sh`
2. Pull Llama 3.1 70B: `ollama pull llama3.1:70b` (takes 2 hours, 40GB download)
3. Install Python dependencies: `pip install langchain langchain-community prometheus-api-client paramiko`
4. Configure Alertmanager webhook to POST alerts to agent endpoint
5. Deploy agent as systemd service or containerized app

**Testing:**
- Start with non-critical alerts
- Compare agent diagnoses to your manual findings
- Build confidence over 20-30 incidents before relying on agent

**Estimated time to working system:** 6-8 hours (setup, config, testing). I spent 12 hours iterating on prompt engineering and tool design.

## Lessons Learned: 3 Months of Autonomous Incident Response

**What worked:**
- Agent dramatically reduced time-to-diagnosis for routine incidents
- Confidence scores helped me decide when to trust agent vs manually verify
- Local LLM inference kept my data private (important for homelab philosophy)
- LangChain's ReAct framework provided good structure for reasoning

**What didn't work:**
- Initial prompt was too vague, agent made wild guesses
- Needed 8 iterations to get prompt specificity right
- Agent hallucinated remediation commands that would have broken the cluster (never auto-remediate without approval)
- False positive rate still 8% (1 in 12 incidents)

**Improvements I'm planning:**
- Add network-layer observability (SNMP, switch metrics)
- Implement feedback loop to store human corrections to failed diagnoses
- Fine-tune Llama 3.1 on my historical incident data (150+ incidents logged)
- Add confidence calibration so agent reports lower confidence on novel incidents

**Would I recommend this?**

Yes, if:
- You're comfortable with LLM limitations
- You have 8+ hours to invest in setup and tuning
- You receive 5+ alerts per week worth automating
- You value your time and sleep

No, if:
- You need 100% accuracy (impossible, agents are probabilistic)
- You can't invest time in prompt engineering and testing
- Your infrastructure is too unique for pattern matching to work
- You prefer manual investigation for learning purposes

## Conclusion: LLMs as SRE Copilots

LLM agents won't replace SREs. But they're excellent copilots for routine incident response. In my homelab, the agent handles 80% of first-pass triage, saving me 4-6 hours per month. The 20% of incidents it misdiagnoses teach me where to improve the system.

**Key takeaways:**
- AIOpsLab framework provides structured approach to evaluating LLM agents for operations
- Local LLM inference (Ollama + Llama 3.1 70B) enables private, cost-effective automation
- Agent reduced my diagnosis time from 30 minutes to 5 minutes average across 12 incidents
- 83% accuracy is good enough for triage, not definitive diagnosis
- Always verify agent findings before taking action

**Next steps I'm exploring:**
- Multi-agent systems: Separate agents for different infrastructure layers (network, compute, storage)
- Proactive anomaly detection: Agent monitors metrics continuously, alerts on deviations before failure
- Remediation automation: Auto-restart pods, scale resources, apply config fixes (with approval gates)

If you're drowning in alerts and spending too much time correlating logs, consider building an LLM agent. Start simple, iterate based on failures, and never fully trust the output. The agent is a tool, not a replacement for human judgment.

Your homelab incidents don't need to wake you at 2 AM. Let the agent handle first-pass triage. You can verify its work in the morning.

## References

1. **[AIOpsLab: A Holistic Framework to Evaluate AI Agents for Enabling Autonomous Clouds](https://arxiv.org/abs/2501.06706)** (2025)
   - Zhao, Y., Banerjee, S., Kalbarczyk, Z., Iyer, R.
   - Microsoft Research

2. **[Prometheus Monitoring](https://prometheus.io/docs/introduction/overview/)** (2024)
   - Cloud Native Computing Foundation
   - Time-series database and alerting system

3. **[Grafana Loki Documentation](https://grafana.com/docs/loki/latest/)** (2024)
   - Grafana Labs
   - Log aggregation system

4. **[LangChain Agent Documentation](https://python.langchain.com/docs/modules/agents/)** (2024)
   - LangChain Development Team
   - Framework for building LLM-powered agents

5. **[Ollama Documentation](https://ollama.ai/docs)** (2024)
   - Ollama Team
   - Local LLM inference platform

6. **[Google SRE Handbook: Monitoring Distributed Systems](https://sre.google/sre-book/monitoring-distributed-systems/)** (2016)
   - Beyer, B., Jones, C., Petoff, J., Murphy, N.
   - O'Reilly Media

7. **[The Four Golden Signals of Observability](https://sre.google/sre-book/monitoring-distributed-systems/#xref_monitoring_golden-signals)** (2016)
   - Google SRE Team
   - Latency, traffic, errors, saturation framework

8. **[Llama 3.1 Technical Report](https://arxiv.org/abs/2407.21783)** (2024)
   - Meta AI
   - 70B parameter open-source language model

9. **[ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629)** (2022)
   - Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., Cao, Y.
   - Framework for LLM reasoning with tool use

10. **[Kubernetes Monitoring Best Practices](https://kubernetes.io/docs/tasks/debug/debug-cluster/resource-metrics-pipeline/)** (2024)
    - Kubernetes Documentation
    - Official monitoring architecture guide

11. **[CoreDNS Documentation](https://coredns.io/manual/toc/)** (2024)
    - CoreDNS Project
    - DNS server for Kubernetes

12. **[K3s Lightweight Kubernetes](https://docs.k3s.io/)** (2024)
    - Rancher Labs (SUSE)
    - Production-ready Kubernetes distribution
